# CVRP-Funsearch

Our project focuses on the Capacitated Vehicle Routing Problem (CVRP), a popular NP-hard problem. This project introduces the innovative FunSearch method to address CVRP, leveraging Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) technology for external knowledge retrieval.

The approach combines deep reasoning through chain of thought to automate program generation and search within the function space. RAG ensures precise knowledge retrieval, providing a theoretical foundation and reasoning framework, while the chain of thought decomposes CVRP into interconnected subproblems for stepwise optimization, tackling single-objective challenges effectively.

**Our goal is simply to minimize the path length, and the following constraints must be satisfied at the same time:**

- All customers are served and each customer is visited only once.
- The capacity of each vehicle is not overloaded.
- No vehicle takes the same path twice.

## Directory Explanation

cvrp-funsearch/

```
│   ├── cvrp
│   │   ├── __init__.py
│   │   ├── data
│   │   │   ├── large
│   │   │   │   ├── ...
│   │   │   └── small
│   │   │       ├── ...
│   │   ├── implementation
│   │   │   ├── README_simple-test.md
│   │   │   ├── __init__.py
│   │   │   ├── code_manipulation.py
│   │   │   ├── cvrp_funsearch.py
│   │   │   ├── evaluator.py
│   │   │   ├── llm_model.py
│   │   │   ├── programs_database.py
│   │   │   ├── prompt_config.py
│   │   │   ├── prompt_generator.py
│   │   │   ├── sampler.py
│   │   │   ├── sand_box.py
│   │   │   ├── sand_box_test.py
│   │   │   └── w2csv.py
│   │   └── spec
│   │       ├── ...
```

Some important directories:

- `data`: Our ddatasets

- `spec`: Specification for different alforithm

- `cvrp/`: Source Code related to solving CVRP solution.

## How to use❕

### ✅ 1. Configure the API Key

Create a `.env` file in the root directory of your project and add `ds` API key to it（in our comment in project）.

```shell
API_KEY = "..."
```

### ✅ Modify the `cvrp_funsearch.py` Script

**2.1 Set the dataset(s) you want to evaluate:**  
Update the line

```python
dataset_names = ['dataset_name']
```

with the dataset names you wish to test.

**2.2 Update the template and dataset paths:**  
Ensure the template file path and dataset file paths point to the correct folders (e.g., `small` or `large`).

**2.3 Set the results output path:**  
Make sure the results are stored in a valid output path (e.g., `results = []` or a designated file location).

### ✅ Run the Script

Execute the script to begin evaluating the selected CVRP dataset(s).

## Design

Our architecture diagram:

<img src="https://github.com/user-attachments/assets/fa80d9ce-0738-4753-a139-bf0b01536a29" width="500px">

Simple explanation:

1. Sampler: Multiple samplers (Sampler 1-5) generate different samples.

2. Request LLM to Generate Code:

- The samplers use an API to send requests to the LLM and obtain multiple (n times) code responses.

- The generated code is based on specific Prompts, which are stored in a Database of prompt templates.

3. Evaluator for Code Evaluation:

- The code generated by the LLM is evaluated in the Evaluator.

- The code runs in a Sandbox environment, where funsearch is used for execution and optimization (run + evolve).

- The evaluation results are ranked in the Score Rank.

4. Select the Best Solution: The best-performing solution from the score ranking is selected as the Final Best Answer.

The core of this process is to generate multiple code solutions using the LLM, execute and evaluate them in a sandbox, and finally choose the optimal solution.

## Dataset

We choose small & medium sets from SetA.

<img src="https://github.com/user-attachments/assets/c94f8976-ff90-4f6a-b4c1-8ea1761a6172" width="500px">

We choose the large set from SetX.

<img src="https://github.com/user-attachments/assets/9915d28d-a4e2-4e26-9f7e-7702784f5996" width="500px">
